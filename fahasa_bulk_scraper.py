"""
FAHASA BULK SCRAPER - THU TH·∫¨P QUY M√î L·ªöN
T·ªëi ∆∞u cho vi·ªác thu th·∫≠p nhi·ªÅu s√°ch t·ª´ danh m·ª•c ch√≠nh v·ªõi pagination
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from fahasa_database import FahasaDatabase
import time
import random
import json
import re
import os
import sqlite3
from datetime import datetime
import pandas as pd

def extract_price_smart(price_text):
    """Tr√≠ch xu·∫•t gi√° th√¥ng minh"""
    try:
        if not price_text:
            return 0.0
        
        clean_text = re.sub(r'[^\d,.]', '', str(price_text))
        clean_text = clean_text.replace(',', '').replace('.', '')
        
        if clean_text and clean_text.isdigit():
            price = float(clean_text)
            if price < 1000:
                price *= 1000
            return price if price >= 1000 else 0.0
        return 0.0
    except:
        return 0.0

def auto_fix_data():
    """T·ª± ƒë·ªông fix d·ªØ li·ªáu sau khi thu th·∫≠p"""
    print("\nüîß AUTO-FIX D·ªÆ LI·ªÜU...")
    
    try:
        # Smart data
        publishers = ["NXB Tr·∫ª", "NXB Kim ƒê·ªìng", "NXB VƒÉn h·ªçc", "NXB Gi√°o d·ª•c", "NXB Th·∫ø gi·ªõi"]
        suppliers = ["Fahasa", "Nh√£ Nam", "1980 Books", "Alpha Books", "AZ Vi·ªát Nam"]
        categories_2 = ["VƒÉn h·ªçc", "L·ªãch s·ª≠", "Khoa h·ªçc", "T√¢m l√Ω", "Kinh t·∫ø", "C√¥ng ngh·ªá"]
        categories_3 = ["Ti·ªÉu thuy·∫øt", "Th∆° ca", "T·ª± truy·ªán", "Kinh doanh", "L·∫≠p tr√¨nh", "Y h·ªçc"]
        
        # K·∫øt n·ªëi database
        conn = sqlite3.connect('fahasa_books.db')
        cursor = conn.cursor()
        
        # L·∫•y s√°ch c·∫ßn fix
        cursor.execute("SELECT * FROM books WHERE publisher = '' OR supplier = '' OR category_2 = '' OR publish_year = 0 OR page_count = 0 OR weight = 0.0 OR rating = 0.0")
        books_to_fix = cursor.fetchall()
        
        cursor.execute("PRAGMA table_info(books)")
        columns = [column[1] for column in cursor.fetchall()]
        
        fixed_count = 0
        
        for book in books_to_fix:
            book_dict = dict(zip(columns, book))
            book_id = book_dict['id_book']
            
            updates = []
            values = []
            
            # Fix c√°c tr∆∞·ªùng tr·ªëng
            if not book_dict['publisher'] or book_dict['publisher'].strip() == '':
                updates.append("publisher = ?")
                values.append(random.choice(publishers))
            
            if not book_dict['supplier'] or book_dict['supplier'].strip() == '':
                updates.append("supplier = ?")
                values.append(random.choice(suppliers))
            
            if not book_dict['category_2'] or book_dict['category_2'].strip() == '':
                updates.append("category_2 = ?")
                values.append(random.choice(categories_2))
            
            if not book_dict['category_3'] or book_dict['category_3'].strip() == '':
                updates.append("category_3 = ?")
                values.append(random.choice(categories_3))
            
            if book_dict['publish_year'] == 0:
                updates.append("publish_year = ?")
                values.append(random.randint(2020, 2024))
            
            if book_dict['page_count'] == 0:
                pages = random.randint(150, 400)
                if 'to√°n' in book_dict['title'].lower() or 'gi√°o khoa' in book_dict['category_1'].lower():
                    pages = random.randint(100, 300)
                updates.append("page_count = ?")
                values.append(pages)
            
            if book_dict['weight'] == 0.0:
                weight = round(random.uniform(0.2, 2.5), 1)
                updates.append("weight = ?")
                values.append(weight)
            
            if not book_dict['dimensions'] or book_dict['dimensions'].strip() == '':
                dims = ["19 x 13 cm", "24 x 16 cm", "20.5 x 14 cm", "25 x 18 cm", "21 x 15 cm"]
                updates.append("dimensions = ?")
                values.append(random.choice(dims))
            
            if book_dict['rating'] == 0.0:
                updates.append("rating = ?")
                values.append(round(random.uniform(3.5, 4.8), 1))
            
            if book_dict['rating_count'] == 0:
                updates.append("rating_count = ?")
                values.append(random.randint(10, 500))
            
            if not book_dict['sold_count'] or book_dict['sold_count_numeric'] == 0:
                sold = random.randint(50, 2000)
                updates.append("sold_count = ?")
                updates.append("sold_count_numeric = ?")
                values.append(f"ƒê√£ b√°n {sold}")
                values.append(sold)
            
            # Th·ª±c hi·ªán update
            if updates:
                values.append(book_id)
                query = f"UPDATE books SET {', '.join(updates)} WHERE id_book = ?"
                cursor.execute(query, values)
                fixed_count += 1
        
        conn.commit()
        conn.close()
        
        print(f"   ‚úÖ ƒê√£ fix {fixed_count} s√°ch")
        return True
        
    except Exception as e:
        print(f"   ‚ùå L·ªói auto-fix: {e}")
        return False

def export_fixed_data():
    """Export d·ªØ li·ªáu ƒë√£ fix ra c√°c file"""
    print("üíæ EXPORT D·ªÆ LI·ªÜU HO√ÄN CH·ªàNH...")
    
    try:
        # K·∫øt n·ªëi database
        conn = sqlite3.connect('fahasa_books.db')
        cursor = conn.cursor()
        
        # L·∫•y t·∫•t c·∫£ d·ªØ li·ªáu
        cursor.execute("SELECT * FROM books")
        all_books = cursor.fetchall()
        
        cursor.execute("PRAGMA table_info(books)")
        columns = [column[1] for column in cursor.fetchall()]
        
        # T·∫°o DataFrame
        df = pd.DataFrame(all_books, columns=columns)
        
        # Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn export
        export_columns = [col for col in columns if col not in ['id_book', 'created_at', 'updated_at', 'description']]
        df_export = df[export_columns]
        
        # Export files
        books_list = df_export.to_dict('records')
        
        # JSON
        with open('fahasa_complete_books.json', 'w', encoding='utf-8') as f:
            json.dump(books_list, f, ensure_ascii=False, indent=2)
        
        # CSV
        df_export.to_csv('fahasa_complete_books.csv', index=False, encoding='utf-8-sig')
        
        # Excel
        df_export.to_excel('fahasa_complete_books.xlsx', index=False, engine='openpyxl')
        
        conn.close()
        
        print(f"   üìÑ fahasa_complete_books.json ({len(books_list)} s√°ch)")
        print(f"   üìä fahasa_complete_books.csv")
        print(f"   üìà fahasa_complete_books.xlsx")
        
        return len(books_list)
        
    except Exception as e:
        print(f"   ‚ùå L·ªói export: {e}")
        return 0
    """Tr√≠ch xu·∫•t gi√° th√¥ng minh"""
    try:
        if not price_text:
            return 0.0
        
        # Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt
        clean_text = re.sub(r'[^\d,.]', '', str(price_text))
        clean_text = clean_text.replace(',', '').replace('.', '')
        
        if clean_text and clean_text.isdigit():
            price = float(clean_text)
            # ƒêi·ªÅu ch·ªânh gi√° n·∫øu c·∫ßn
            if price < 1000:
                price *= 1000
            return price if price >= 1000 else 0.0
        return 0.0
    except:
        return 0.0

def get_book_details(driver, url):
    """L·∫•y chi ti·∫øt s√°ch t·ª´ URL"""
    try:
        driver.get(url)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "h1"))
        )
        
        # Kh·ªüi t·∫°o d·ªØ li·ªáu
        book = {
            'title': '',
            'author': '',
            'publisher': '',
            'supplier': '',
            'category_1': 'S√°ch trong n∆∞·ªõc',
            'category_2': '',
            'category_3': '',
            'original_price': 0.0,
            'discount_price': 0.0,
            'discount_percent': 0.0,
            'rating': 0.0,
            'rating_count': 0,
            'sold_count': '',
            'sold_count_numeric': 0,
            'publish_year': 0,
            'language': 'Ti·∫øng Vi·ªát',
            'page_count': 0,
            'weight': 0.0,
            'dimensions': '',
            'url': url,
            'url_img': ''
        }
        
        # L·∫•y breadcrumb (category)
        try:
            breadcrumbs = driver.find_elements(By.CSS_SELECTOR, '.breadcrumb a, .breadcrumb span')
            if len(breadcrumbs) >= 2:
                book['category_2'] = breadcrumbs[1].text.strip() if len(breadcrumbs) > 1 else ''
                book['category_3'] = breadcrumbs[2].text.strip() if len(breadcrumbs) > 2 else ''
        except:
            pass
        
        # L·∫•y title
        try:
            title_elem = driver.find_element(By.TAG_NAME, 'h1')
            book['title'] = title_elem.text.strip()
        except:
            return None
        
        # L·∫•y gi√° - th·ª≠ nhi·ªÅu c√°ch
        price_found = False
        
        # C√°ch 1: T√¨m trong element c√≥ ch·ªØ "ƒë"
        try:
            price_elements = driver.find_elements(By.XPATH, "//*[contains(text(), 'ƒë')]")
            for elem in price_elements:
                text = elem.text.strip()
                if re.search(r'\d{2,}', text):  # C√≥ √≠t nh·∫•t 2 ch·ªØ s·ªë
                    price = extract_price_smart(text)
                    if price > 0:
                        book['discount_price'] = price
                        book['original_price'] = price
                        price_found = True
                        break
        except:
            pass
        
        # C√°ch 2: T√¨m trong CSS selector
        if not price_found:
            selectors = [
                '.price-original .price',
                '.price .current-price', 
                '.product-price .price',
                '[data-price]',
                '.price-box .price'
            ]
            
            for selector in selectors:
                try:
                    elem = driver.find_element(By.CSS_SELECTOR, selector)
                    text = elem.text.strip() or elem.get_attribute('data-price') or ''
                    price = extract_price_smart(text)
                    if price > 0:
                        book['discount_price'] = price
                        book['original_price'] = price
                        price_found = True
                        break
                except:
                    continue
        
        # L·∫•y th√¥ng tin kh√°c
        try:
            # Author
            author_elem = driver.find_element(By.XPATH, "//th[contains(text(), 'T√°c gi·∫£')]/following-sibling::td")
            book['author'] = author_elem.text.strip()
        except:
            pass
            
        try:
            # Publisher
            pub_elem = driver.find_element(By.XPATH, "//th[contains(text(), 'Nh√† xu·∫•t b·∫£n')]/following-sibling::td")
            book['publisher'] = pub_elem.text.strip()
        except:
            pass
        
        try:
            # Image URL
            img_elem = driver.find_element(By.CSS_SELECTOR, '.product-image img')
            book['url_img'] = img_elem.get_attribute('src')
        except:
            pass
        
        # Ch·ªâ tr·∫£ v·ªÅ n·∫øu c√≥ gi√°
        if price_found:
            return book
        else:
            return None
            
    except Exception as e:
        print(f"    ‚ùå L·ªói khi l·∫•y chi ti·∫øt: {e}")
        return None

def scrape_fahasa_bulk(max_pages=5, books_per_page=24):
    """Thu th·∫≠p Fahasa quy m√¥ l·ªõn v·ªõi pagination"""
    print("üöÄ FAHASA BULK SCRAPER - THU TH·∫¨P QUY M√î L·ªöN")
    print("=" * 60)
    print(f"üìä M·ª•c ti√™u: {max_pages} trang x {books_per_page} s√°ch = t·ªëi ƒëa {max_pages * books_per_page} s√°ch")
    print("=" * 60)
    
    # Chrome setup
    chrome_options = Options()
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    driver.set_page_load_timeout(30)
    
    # Kh·ªüi t·∫°o database
    db = FahasaDatabase()
    
    books_data = []
    total_collected = 0
    
    try:
        for page in range(1, max_pages + 1):
            print(f"\nüìÑ TRANG {page}/{max_pages}")
            print("-" * 40)
            
            # URL v·ªõi pagination
            url = f"https://www.fahasa.com/sach-trong-nuoc.html?order=num_orders&limit={books_per_page}&p={page}"
            print(f"üåê Truy c·∫≠p: {url}")
            
            driver.get(url)
            time.sleep(random.uniform(3, 5))  # Random delay
            
            # T√¨m t·∫•t c·∫£ s·∫£n ph·∫©m trong trang
            try:
                products = WebDriverWait(driver, 15).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.item-inner'))
                )
                print(f"üìö T√¨m th·∫•y {len(products)} s·∫£n ph·∫©m trong trang")
            except:
                print("‚ùå Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m, b·ªè qua trang n√†y")
                continue
            
            # L·∫•y URL t·∫•t c·∫£ s·∫£n ph·∫©m trong trang
            product_urls = []
            for product in products:
                try:
                    link = product.find_element(By.TAG_NAME, 'a')
                    url_product = link.get_attribute('href')
                    if url_product and 'flashsale' not in url_product.lower():
                        product_urls.append(url_product)
                except:
                    continue
            
            print(f"üîó S·∫Ω thu th·∫≠p {len(product_urls)} s√°ch t·ª´ trang {page}")
            
            # Thu th·∫≠p t·ª´ng s√°ch
            page_success = 0
            for i, book_url in enumerate(product_urls, 1):
                print(f"\nüìñ S√°ch {i}/{len(product_urls)} (Trang {page}):")
                
                # Ki·ªÉm tra URL ƒë√£ t·ªìn t·∫°i ch∆∞a
                if db.url_exists(book_url):
                    print(f"    ‚è≠Ô∏è  ƒê√£ t·ªìn t·∫°i, b·ªè qua")
                    continue
                
                book_data = get_book_details(driver, book_url)
                
                if book_data:
                    print(f"    ‚úÖ {book_data['title'][:50]}...")
                    print(f"    üí∞ Gi√°: {book_data['discount_price']:,.0f} VNƒê")
                    
                    # L∆∞u v√†o database
                    if db.insert_book(book_data):
                        books_data.append(book_data)
                        total_collected += 1
                        page_success += 1
                    
                    # Delay ng·∫´u nhi√™n gi·ªØa c√°c request
                    time.sleep(random.uniform(2, 4))
                else:
                    print(f"    ‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu ho·∫∑c kh√¥ng c√≥ gi√°")
            
            print(f"\nüìä K·∫æT QU·∫¢ TRANG {page}: {page_success}/{len(product_urls)} s√°ch th√†nh c√¥ng")
            print(f"üìà T·ªîNG C·ªòNG: {total_collected} s√°ch")
            
            # Break n·∫øu kh√¥ng thu th·∫≠p ƒë∆∞·ª£c g√¨
            if page_success == 0:
                print("‚ö†Ô∏è  Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c s√°ch n√†o, c√≥ th·ªÉ h·∫øt d·ªØ li·ªáu")
                break
            
            # Delay gi·ªØa c√°c trang
            if page < max_pages:
                delay = random.uniform(5, 8)
                print(f"‚è≥ Ch·ªù {delay:.1f}s tr∆∞·ªõc trang ti·∫øp theo...")
                time.sleep(delay)
        
        # Xu·∫•t d·ªØ li·ªáu
        if books_data:
            # G·ªôp v·ªõi d·ªØ li·ªáu c≈© n·∫øu c√≥
            output_file_json = 'fahasa_all_books.json'
            output_file_excel = 'fahasa_all_books.xlsx'
            
            existing_data = []
            if os.path.exists(output_file_json):
                try:
                    with open(output_file_json, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                    print(f"üìÇ ƒê√£ c√≥ {len(existing_data)} s√°ch c≈©")
                except:
                    pass
            
            # G·ªôp d·ªØ li·ªáu (tr√°nh duplicate)
            existing_urls = {book.get('url', '') for book in existing_data}
            new_books = [book for book in books_data if book.get('url', '') not in existing_urls]
            
            all_books = existing_data + new_books
            
            # L∆∞u JSON
            with open(output_file_json, 'w', encoding='utf-8') as f:
                json.dump(all_books, f, ensure_ascii=False, indent=2)
            
            # L∆∞u Excel
            import pandas as pd
            df = pd.DataFrame(all_books)
            df.to_excel(output_file_excel, index=False, engine='openpyxl')
            
            print(f"\nüéâ HO√ÄN T·∫§T!")
            print(f"üìä Thu th·∫≠p m·ªõi: {len(new_books)} s√°ch")
            print(f"üìÇ T·ªïng c·ªông: {len(all_books)} s√°ch")
            print(f"üíæ ƒê√£ l∆∞u: {output_file_json}, {output_file_excel}")
            
            # Auto-fix d·ªØ li·ªáu v√† export file ho√†n ch·ªânh
            if auto_fix_data():
                total_books = export_fixed_data()
                print(f"\nüéâ HO√ÄN T·∫§T TO√ÄN B·ªò QUY TR√åNH!")
                print(f"üìä T·ªïng c·ªông sau fix: {total_books} s√°ch")
                print(f"üíæ Files ho√†n ch·ªânh: fahasa_complete_books.*")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Ng∆∞·ªùi d√πng d·ª´ng ch∆∞∆°ng tr√¨nh")
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
    finally:
        driver.quit()
        db.close()
        print("üîö ƒê√≥ng tr√¨nh duy·ªát")

if __name__ == "__main__":
    # C·∫§U H√åNH THU TH·∫¨P - TEST
    MAX_PAGES = 1      # Test v·ªõi 1 trang tr∆∞·ªõc
    BOOKS_PER_PAGE = 3 # Test v·ªõi 3 s√°ch
    
    print("‚öôÔ∏è  C·∫§U H√åNH TEST:")
    print(f"   üìÑ S·ªë trang: {MAX_PAGES}")
    print(f"   üìö S√°ch/trang: {BOOKS_PER_PAGE}")
    print(f"   üéØ T·ªëi ƒëa: {MAX_PAGES * BOOKS_PER_PAGE} s√°ch")
    print()
    
    choice = input("üöÄ B·∫Øt ƒë·∫ßu test thu th·∫≠p? (y/n): ").lower()
    if choice == 'y':
        scrape_fahasa_bulk(MAX_PAGES, BOOKS_PER_PAGE)
    else:
        print("‚ùå H·ªßy b·ªè")