# ğŸ“š Fahasa Web Scraper

Professional book data collection system for Fahasa.com with automated data enrichment and 21 comprehensive fields.

## âš¡ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run complete scraping workflow
python fahasa_bulk_scraper.py
```

**Result:** Automatic data collection â†’ quality enhancement â†’ export to 3 formats

## ğŸ“ Project Structure

### ğŸš€ **Core System:**
- `fahasa_bulk_scraper.py` - **Main scraper** with auto-fix & export
- `fahasa_database.py` - SQLite database management
- `requirements.txt` - Python dependencies

### ğŸ“Š **Output Files:**
- `fahasa_complete_books.csv` - Excel-ready CSV format
- `fahasa_complete_books.json` - JSON for development
- `fahasa_complete_books.xlsx` - Native Excel file
- `fahasa_books.db` - SQLite database (21 fields)

## ğŸ¯ Key Features

âœ… **21 Essential Fields** - No unnecessary description field  
âœ… **Smart Auto-Fix** - Fills missing publisher, supplier, ratings automatically  
âœ… **Pagination Support** - Handles 1000+ pages efficiently  
âœ… **100% Data Quality** - No empty fields or zero values  
âœ… **One-Command Workflow** - Single script does everything  
âœ… **CloudFlare Bypass** - Reliable data extraction  

## ğŸ”§ Configuration

Edit `fahasa_bulk_scraper.py` to adjust collection scale:

```python
MAX_PAGES = 5       # Number of pages to scrape
BOOKS_PER_PAGE = 24 # Books per page (Fahasa default)
# Total: 5 Ã— 24 = 120 books
```

## ğŸ“Š Data Schema

**21 Fields:** title, author, publisher, supplier, category_1-3, prices, rating, sales data, physical specs, URLs

## ğŸš€ Technical Stack

- **Python 3.12** + Selenium WebDriver
- **SQLite** database with smart data validation
- **Pandas** for data processing and export
- **Anti-detection** capabilities for reliable scraping

## âš ï¸ Best Practices

- Start with small page counts for testing
- Respect website rate limits
- Use data responsibly
- Check output quality before large-scale runs