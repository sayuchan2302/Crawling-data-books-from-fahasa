#!/usr/bin/env python3
"""
Full Crawler and Load Workflow
1. Cháº¡y crawler â†’ táº¡o CSV/JSON files
2. Load CSV vÃ o staging table
"""
import subprocess
import sys
import os
from datetime import datetime

def run_command(command, description):
    """Cháº¡y command vÃ  hiá»ƒn thá»‹ káº¿t quáº£"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ {description}")
    print(f"{'='*60}")
    
    try:
        # Set UTF-8 encoding for subprocess
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        
        result = subprocess.run(
            command, 
            shell=True, 
            capture_output=True, 
            text=True, 
            timeout=300,
            env=env,
            encoding='utf-8'
        )
        
        if result.stdout:
            print("ğŸ“¤ OUTPUT:")
            print(result.stdout)
        
        if result.stderr and result.returncode != 0:
            print("ğŸ“¥ STDERR:")
            print(result.stderr)
        
        if result.returncode == 0:
            print(f"âœ… {description} - THÃ€NH CÃ”NG")
            return True
        else:
            print(f"âŒ {description} - THáº¤T Báº I (exit code: {result.returncode})")
            return False
            
    except subprocess.TimeoutExpired:
        print(f"â° {description} - TIMEOUT (5 phÃºt)")
        return False
    except Exception as e:
        print(f"âŒ Lá»—i cháº¡y command: {e}")
        return False

def main():
    """Main workflow"""
    print("ğŸ”¥ FAHASA DATA WAREHOUSE - CLEAN WORKFLOW")
    print("=" * 60)
    print("ğŸ“‹ WORKFLOW:")
    print("   1ï¸âƒ£ Crawler â†’ Táº¡o CSV/JSON files")
    print("   2ï¸âƒ£ CSV Loader â†’ Load vÃ o staging table")
    print("=" * 60)
    
    # Change to script directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)
    
    print(f"ğŸ“ Working directory: {os.getcwd()}")
    
    # Step 1: Run Crawler (file-only mode)
    print("\nğŸ•·ï¸ BÆ¯á»šC 1: Chá»‰ táº¡o files, khÃ´ng insert database")
    crawler_success = run_command(
        "python src/crawler/fahasa_bulk_scraper.py",
        "Crawler - Táº¡o CSV/JSON files"
    )
    
    if not crawler_success:
        print("âŒ Crawler tháº¥t báº¡i, dá»«ng workflow")
        return False
    
    # Step 2: Load CSV to Staging
    print("\nğŸ“Š BÆ¯á»šC 2: Load CSV vÃ o staging table")
    load_success = run_command(
        "python src/etl/load_csv_to_staging.py",
        "Load CSV â†’ MySQL staging_books"
    )
    
    if not load_success:
        print("âŒ Load CSV tháº¥t báº¡i")
        return False
    
    # Step 3: Show Summary
    print(f"\n{'='*60}")
    print("ğŸ“Š TÃ“M Táº®T CLEAN WORKFLOW")
    print(f"{'='*60}")
    
    # Get current file info
    now = datetime.now()
    backup_dir = os.path.join('data', str(now.year), f"{now.month:02d}", f"{now.day:02d}")
    
    if os.path.exists(backup_dir):
        csv_files = [f for f in os.listdir(backup_dir) if f.endswith('.csv')]
        json_files = [f for f in os.listdir(backup_dir) if f.endswith('.json')]
        
        if csv_files:
            csv_files.sort(reverse=True)
            json_files.sort(reverse=True)
            print(f"ğŸ“ Files created:")
            print(f"   CSV: {os.path.join(backup_dir, csv_files[0])}")
            print(f"   JSON: {os.path.join(backup_dir, json_files[0])}")
    
    print("\nâœ… CLEAN WORKFLOW HOÃ€N THÃ€NH!")
    print("\nğŸ”„ WORKFLOW TIáº¾P THEO:")
    print("   3ï¸âƒ£ ETL: staging â†’ Data Warehouse")
    print("   4ï¸âƒ£ Aggregate â†’ datamart")
    print("   5ï¸âƒ£ Data quality checks")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)